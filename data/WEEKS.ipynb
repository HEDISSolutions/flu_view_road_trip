{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file: C:\\Users\\board\\Proj3 exfiles\\week_10.csv\n",
      "Created file: C:\\Users\\board\\Proj3 exfiles\\week_11.csv\n",
      "Created file: C:\\Users\\board\\Proj3 exfiles\\week_12.csv\n",
      "Created file: C:\\Users\\board\\Proj3 exfiles\\week_13.csv\n",
      "Created file: C:\\Users\\board\\Proj3 exfiles\\week_14.csv\n",
      "Created file: C:\\Users\\board\\Proj3 exfiles\\week_15.csv\n",
      "Created file: C:\\Users\\board\\Proj3 exfiles\\week_16.csv\n",
      "Created file: C:\\Users\\board\\Proj3 exfiles\\week_17.csv\n",
      "Created file: C:\\Users\\board\\Proj3 exfiles\\week_18.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the correct file path\n",
    "file_path = r\"C:\\Users\\board\\Proj3 exfiles\\updated_route_to_grandmas.csv\"  # Use the updated absolute path\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the 'WEEK' column is properly formatted\n",
    "if \"WEEK\" not in data.columns:\n",
    "    raise ValueError(\"The CSV file must contain a 'WEEK' column.\")\n",
    "\n",
    "# Output directory (same as input file's directory)\n",
    "output_directory = os.path.dirname(file_path)\n",
    "\n",
    "# Group by 'WEEK' and save each group as a separate CSV file\n",
    "for week, group in data.groupby(\"WEEK\"):\n",
    "    output_file = os.path.join(output_directory, f\"week_{week}.csv\".replace(\"/\", \"-\"))  # Replace slashes in week\n",
    "    group.to_csv(output_file, index=False)\n",
    "    print(f\"Created file: {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Sights\\tLatitude\\tLongitude\\tStart\\tFinish'],\n",
       " ['CBSA_ID',\n",
       "  'CBSA_NAME',\n",
       "  'ACUITY_LEVEL',\n",
       "  'WEEK',\n",
       "  'SEASON',\n",
       "  'Latitude',\n",
       "  'Longitude',\n",
       "  'ACTIVITY_LEVEL'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the files with correct paths\n",
    "scenic_sites_file = r\"C:\\Users\\board\\PROJECT_3_FLUVIEW\\data\\scenic_sites_itinerary.csv\"\n",
    "route_to_grandmas_file = r\"C:\\Users\\board\\PROJECT_3_FLUVIEW\\data\\updated_route_to_grandmas.csv\"\n",
    "\n",
    "# Load the datasets to check their columns\n",
    "scenic_sites_df = pd.read_csv(scenic_sites_file)\n",
    "route_to_grandmas_df = pd.read_csv(route_to_grandmas_file)\n",
    "\n",
    "# Display column names for both datasets\n",
    "scenic_sites_columns = scenic_sites_df.columns.tolist()\n",
    "route_to_grandmas_columns = route_to_grandmas_df.columns.tolist()\n",
    "\n",
    "scenic_sites_columns, route_to_grandmas_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenic Sites Rows: 15\n",
      "Route to Grandma's Rows: 1266\n"
     ]
    }
   ],
   "source": [
    "print(\"Scenic Sites Rows:\", len(scenic_sites_df))\n",
    "print(\"Route to Grandma's Rows:\", len(route_to_grandmas_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Lat/Lon in Scenic Sites:\n",
      "    Latitude  Longitude\n",
      "0    32.7804  -106.1714\n",
      "1    15.1556   145.7068\n",
      "2    29.6571   -98.6117\n",
      "3    30.1987   -97.8313\n",
      "4    28.3852   -81.5639\n",
      "5    28.0419   -82.4150\n",
      "6    28.0365   -82.4151\n",
      "7    27.9244   -82.8410\n",
      "8    29.9604   -90.0589\n",
      "9    29.9471   -90.0918\n",
      "10   31.8993   -88.3112\n",
      "11   30.3496   -87.3035\n",
      "12   25.0273   -81.5358\n",
      "13   34.0522  -118.2437\n",
      "14   25.7907   -80.1300\n",
      "Unique Lat/Lon in Route to Grandma's:\n",
      "     Latitude  Longitude\n",
      "0     32.8995  -105.9603\n",
      "1     34.2676   -86.2089\n",
      "2     35.0844  -106.6504\n",
      "3     32.9440   -85.9539\n",
      "4     31.3113   -92.4451\n",
      "..        ...        ...\n",
      "136   28.8053   -97.0036\n",
      "137   31.5493   -97.1467\n",
      "138   27.5473   -81.8115\n",
      "139   39.1407  -121.6177\n",
      "140   32.6927  -114.6277\n",
      "\n",
      "[141 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Lat/Lon in Scenic Sites:\")\n",
    "print(scenic_sites_df[['Latitude', 'Longitude']].drop_duplicates())\n",
    "\n",
    "print(\"Unique Lat/Lon in Route to Grandma's:\")\n",
    "print(route_to_grandmas_df[['Latitude', 'Longitude']].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenic Sites Columns: Index(['Sights', 'Latitude', 'Longitude', 'Start', 'Finish'], dtype='object')\n",
      "Route Columns: Index(['CBSA_ID', 'CBSA_NAME', 'ACUITY_LEVEL', 'WEEK', 'SEASON', 'Latitude',\n",
      "       'Longitude', 'ACTIVITY_LEVEL'],\n",
      "      dtype='object')\n",
      "Unique Lat/Lon in Scenic Sites:\n",
      "    Latitude_Rounded  Longitude_Rounded\n",
      "0            32.7804          -106.1714\n",
      "1            15.1556           145.7068\n",
      "2            29.6571           -98.6117\n",
      "3            30.1987           -97.8313\n",
      "4            28.3852           -81.5639\n",
      "5            28.0419           -82.4150\n",
      "6            28.0365           -82.4151\n",
      "7            27.9244           -82.8410\n",
      "8            29.9604           -90.0589\n",
      "9            29.9471           -90.0918\n",
      "10           31.8993           -88.3112\n",
      "11           30.3496           -87.3035\n",
      "12           25.0273           -81.5358\n",
      "13           34.0522          -118.2437\n",
      "14           25.7907           -80.1300\n",
      "Unique Lat/Lon in Route Data:\n",
      "     Latitude_Rounded  Longitude_Rounded\n",
      "0             32.8995          -105.9603\n",
      "1             34.2676           -86.2089\n",
      "2             35.0844          -106.6504\n",
      "3             32.9440           -85.9539\n",
      "4             31.3113           -92.4451\n",
      "..                ...                ...\n",
      "136           28.8053           -97.0036\n",
      "137           31.5493           -97.1467\n",
      "138           27.5473           -81.8115\n",
      "139           39.1407          -121.6177\n",
      "140           32.6927          -114.6277\n",
      "\n",
      "[141 rows x 2 columns]\n",
      "No matching rows found during merge.\n",
      "Merged data saved to: C:\\Users\\board\\PROJECT_3_FLUVIEW\\data\\merged_scenic_and_route_safe.csv\n",
      "Preview of Merged Data:\n",
      "Empty DataFrame\n",
      "Columns: [Sights, Latitude_scenic, Longitude_scenic, Start, Finish, Latitude_Rounded, Longitude_Rounded, CBSA_ID, CBSA_NAME, ACUITY_LEVEL, WEEK, SEASON, Latitude_route, Longitude_route, ACTIVITY_LEVEL, SITES]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "scenic_sites_file = r\"C:\\Users\\board\\PROJECT_3_FLUVIEW\\data\\scenic_sites_itinerary.csv\"\n",
    "route_to_grandmas_file = r\"C:\\Users\\board\\PROJECT_3_FLUVIEW\\data\\updated_route_to_grandmas.csv\"\n",
    "\n",
    "# Load datasets\n",
    "scenic_sites_df = pd.read_csv(scenic_sites_file, delimiter=\"\\t\").copy()\n",
    "route_to_grandmas_df = pd.read_csv(route_to_grandmas_file).copy()\n",
    "\n",
    "# Ensure column names match\n",
    "print(\"Scenic Sites Columns:\", scenic_sites_df.columns)\n",
    "print(\"Route Columns:\", route_to_grandmas_df.columns)\n",
    "\n",
    "# Apply rounding for better matching\n",
    "scenic_sites_df['Latitude_Rounded'] = scenic_sites_df['Latitude'].round(4)\n",
    "scenic_sites_df['Longitude_Rounded'] = scenic_sites_df['Longitude'].round(4)\n",
    "route_to_grandmas_df['Latitude_Rounded'] = route_to_grandmas_df['Latitude'].round(4)\n",
    "route_to_grandmas_df['Longitude_Rounded'] = route_to_grandmas_df['Longitude'].round(4)\n",
    "\n",
    "# Check unique Lat/Lon values before merge\n",
    "print(\"Unique Lat/Lon in Scenic Sites:\")\n",
    "print(scenic_sites_df[['Latitude_Rounded', 'Longitude_Rounded']].drop_duplicates())\n",
    "\n",
    "print(\"Unique Lat/Lon in Route Data:\")\n",
    "print(route_to_grandmas_df[['Latitude_Rounded', 'Longitude_Rounded']].drop_duplicates())\n",
    "\n",
    "# Merge datasets\n",
    "merged_df = pd.merge(\n",
    "    scenic_sites_df,\n",
    "    route_to_grandmas_df,\n",
    "    on=[\"Latitude_Rounded\", \"Longitude_Rounded\"],\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_scenic\", \"_route\")\n",
    ")\n",
    "\n",
    "# Check if the merge worked\n",
    "if merged_df.empty:\n",
    "    print(\"No matching rows found during merge.\")\n",
    "else:\n",
    "    print(\"Merged Rows:\", len(merged_df))\n",
    "\n",
    "# Combine site names for matched rows\n",
    "if 'Sights' in merged_df.columns and 'CBSA_NAME' in merged_df.columns:\n",
    "    merged_df[\"SITES\"] = merged_df[[\"Sights\", \"CBSA_NAME\"]].apply(\n",
    "        lambda x: ', '.join(x.dropna()), axis=1\n",
    "    )\n",
    "\n",
    "# Save the merged file to a new location to avoid overwriting\n",
    "output_file = r\"C:\\Users\\board\\PROJECT_3_FLUVIEW\\data\\merged_scenic_and_route_safe.csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Display results\n",
    "print(f\"Merged data saved to: {output_file}\")\n",
    "print(\"Preview of Merged Data:\")\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correct local Windows paths\n",
    "sites_path = r'C:\\Users\\board\\PROJECT_3_FLUVIEW\\data\\sites.csv'\n",
    "route_path = r'C:\\Users\\board\\PROJECT_3_FLUVIEW\\data\\updated_route_to_grandmas.csv'\n",
    "\n",
    "# Load the data\n",
    "sites_df = pd.read_csv(sites_path)\n",
    "route_df = pd.read_csv(route_path)\n",
    "\n",
    "# Here you would include any data processing or merging operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                           Sites   Latitude   Longitude\n",
       " 0  White Sands National Park\\t\\t  32.780361 -106.171448\n",
       " 1                Texas Capitol\\t  15.155630 -106.171448\n",
       " 2               The Galleria\\t\\t  29.657089  -98.611732\n",
       " 3     Oak Alley Plantation\\t\\t\\t  30.198700  -97.831329\n",
       " 4            DisneyWorld\\t\\t\\t\\t  28.385233  -81.563873,\n",
       "    CBSA_ID           CBSA_NAME  ACUITY_LEVEL      WEEK   SEASON   Latitude  \\\n",
       " 0      454      Alamogordo, NM             7  3/9/2024  2023-24  32.899532   \n",
       " 1      250     Albertville, AL             3  3/9/2024  2023-24  34.267594   \n",
       " 2      165     Albuquerque, NM            11  3/9/2024  2023-24  35.084386   \n",
       " 3      891  Alexander City, AL             2  3/9/2024  2023-24  32.944012   \n",
       " 4       66      Alexandria, LA             5  3/9/2024  2023-24  31.311294   \n",
       " \n",
       "     Longitude ACTIVITY_LEVEL  \n",
       " 0 -105.960265       Moderate  \n",
       " 1  -86.208867        Minimal  \n",
       " 2 -106.650422      Very High  \n",
       " 3  -85.953853        Minimal  \n",
       " 4  -92.445137            Low  )"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites_df.head(), route_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sites</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>White Sands National Park</td>\n",
       "      <td>32.839947</td>\n",
       "      <td>-106.065856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Texas Capitol</td>\n",
       "      <td>20.767709</td>\n",
       "      <td>-102.495876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Galleria</td>\n",
       "      <td>29.541005</td>\n",
       "      <td>-98.552400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oak Alley Plantation</td>\n",
       "      <td>30.353478</td>\n",
       "      <td>-97.755112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DisneyWorld</td>\n",
       "      <td>28.484024</td>\n",
       "      <td>-81.451907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Universal Islands of Adventure</td>\n",
       "      <td>28.158129</td>\n",
       "      <td>-82.401774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Busch Gardens Tampa Bay</td>\n",
       "      <td>28.155418</td>\n",
       "      <td>-82.401829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Clearwater Beach</td>\n",
       "      <td>28.099393</td>\n",
       "      <td>-82.614791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>French Quarter</td>\n",
       "      <td>29.981177</td>\n",
       "      <td>-90.111125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jackson Square</td>\n",
       "      <td>29.974532</td>\n",
       "      <td>-90.127599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>USS ALABAMA Battleship Memorial Park</td>\n",
       "      <td>31.461537</td>\n",
       "      <td>-87.902556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>National Naval Aviation Museum</td>\n",
       "      <td>30.409194</td>\n",
       "      <td>-87.269814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Florida Keys</td>\n",
       "      <td>24.791198</td>\n",
       "      <td>-81.657869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>33.962418</td>\n",
       "      <td>-118.264245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GRANDMAS HOUSE</td>\n",
       "      <td>26.252553</td>\n",
       "      <td>-80.092494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Sites   Latitude   Longitude\n",
       "0              White Sands National Park  32.839947 -106.065856\n",
       "1                          Texas Capitol  20.767709 -102.495876\n",
       "2                           The Galleria  29.541005  -98.552400\n",
       "3                   Oak Alley Plantation  30.353478  -97.755112\n",
       "4                            DisneyWorld  28.484024  -81.451907\n",
       "5         Universal Islands of Adventure  28.158129  -82.401774\n",
       "6                Busch Gardens Tampa Bay  28.155418  -82.401829\n",
       "7                       Clearwater Beach  28.099393  -82.614791\n",
       "8                         French Quarter  29.981177  -90.111125\n",
       "9                         Jackson Square  29.974532  -90.127599\n",
       "10  USS ALABAMA Battleship Memorial Park  31.461537  -87.902556\n",
       "11        National Naval Aviation Museum  30.409194  -87.269814\n",
       "12                          Florida Keys  24.791198  -81.657869\n",
       "13                           Los Angeles  33.962418 -118.264245\n",
       "14                        GRANDMAS HOUSE  26.252553  -80.092494"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up site names by removing extraneous tab characters\n",
    "sites_df['Sites'] = sites_df['Sites'].str.replace(r'\\t+', '', regex=True)\n",
    "\n",
    "# Define a function to find the closest point in route_df for each point in sites_df and calculate the median coordinates\n",
    "def adjust_coordinates(sites, route):\n",
    "    # Prepare columns to store adjusted latitudes and longitudes\n",
    "    adjusted_lats = []\n",
    "    adjusted_lons = []\n",
    "\n",
    "    # For each site, find the closest route point\n",
    "    for index, site in sites.iterrows():\n",
    "        # Calculate the distance to each route point\n",
    "        distances = np.sqrt((route['Latitude'] - site['Latitude'])**2 + (route['Longitude'] - site['Longitude'])**2)\n",
    "        closest_idx = distances.idxmin()  # Index of the closest route point\n",
    "\n",
    "        # Calculate median latitude and longitude\n",
    "        median_lat = np.median([site['Latitude'], route.loc[closest_idx, 'Latitude']])\n",
    "        median_lon = np.median([site['Longitude'], route.loc[closest_idx, 'Longitude']])\n",
    "\n",
    "        # Append adjusted coordinates\n",
    "        adjusted_lats.append(median_lat)\n",
    "        adjusted_lons.append(median_lon)\n",
    "\n",
    "    # Update sites dataframe with adjusted coordinates\n",
    "    sites['Latitude'] = adjusted_lats\n",
    "    sites['Longitude'] = adjusted_lons\n",
    "\n",
    "    return sites\n",
    "\n",
    "# Adjust the coordinates in sites_df based on the closest matches in route_df\n",
    "adjusted_sites_df = adjust_coordinates(sites_df, route_df)\n",
    "adjusted_sites_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dallen)",
   "language": "python",
   "name": "dallen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
